"use strict";(self.webpackChunklimo_cobot_documentation=self.webpackChunklimo_cobot_documentation||[]).push([[3164],{44700:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>h,contentTitle:()=>d,default:()=>x,frontMatter:()=>a,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"core-concepts/object-detection","title":"Object Detection Pipeline","description":"Object detection enables your robot to see and identify objects in the real world, so it can pick, avoid, or interact with them.","source":"@site/docs/core-concepts/object-detection.md","sourceDirName":"core-concepts","slug":"/core-concepts/object-detection","permalink":"/limo_cobot_documentation/docs/core-concepts/object-detection","draft":false,"unlisted":false,"editUrl":"https://github.com/krish-rRay23/limo_cobot_documentation/tree/main/docs/core-concepts/object-detection.md","tags":[],"version":"current","lastUpdatedBy":"Krish Ray","lastUpdatedAt":1754637033000,"sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"Object Detection Pipeline"},"sidebar":"tutorialSidebar","previous":{"title":"Manipulation Basics","permalink":"/limo_cobot_documentation/docs/core-concepts/manipulation"},"next":{"title":"System Components Overview","permalink":"/limo_cobot_documentation/docs/system-components/overview"}}');var i=r(74848),s=r(28453),o=r(27293),c=r(11470),l=r(19365);const a={sidebar_position:6,title:"Object Detection Pipeline"},d="\ud83d\udc41\ufe0f Object Detection Pipeline",h={},p=[{value:"\ud83e\udde0 Object Detection Fundamentals",id:"-object-detection-fundamentals",level:2},{value:"<strong>Why Object Detection Matters</strong>",id:"why-object-detection-matters",level:3},{value:"\u26a1 YOLO Implementation",id:"-yolo-implementation",level:2},{value:"<strong>Why YOLOv8?</strong>",id:"why-yolov8",level:3},{value:"<strong>Detection Pipeline Architecture</strong>",id:"detection-pipeline-architecture",level:3},{value:"\ud83c\udfaf Training Custom Models",id:"-training-custom-models",level:2},{value:"<strong>Data Collection</strong>",id:"data-collection",level:3},{value:"<strong>Model Deployment</strong>",id:"model-deployment",level:3},{value:"\ud83d\udd17 Detection Pipeline Integration",id:"-detection-pipeline-integration",level:2},{value:"<strong>Message Definitions</strong>",id:"message-definitions",level:3},{value:"<strong>3D Pose Estimation</strong>",id:"3d-pose-estimation",level:3},{value:"\ud83d\udcca Performance Optimization",id:"-performance-optimization",level:2},{value:"<strong>Inference Speed</strong>",id:"inference-speed",level:3},{value:"<strong>Memory Management</strong>",id:"memory-management",level:3},{value:"\ud83d\udee0\ufe0f Troubleshooting &amp; Best Practices",id:"\ufe0f-troubleshooting--best-practices",level:2},{value:"<strong>Common Issues</strong>",id:"common-issues",level:3},{value:"\ud83d\udcc8 Advanced Features",id:"-advanced-features",level:2},{value:"<strong>Object Tracking</strong>",id:"object-tracking",level:3},{value:"<strong>Multi-Class Strategy</strong>",id:"multi-class-strategy",level:3},{value:"\ud83d\udcca Output Topics &amp; Integration",id:"-output-topics--integration",level:2},{value:"<strong>Published Topics</strong>",id:"published-topics",level:3},{value:"<strong>Integration Example</strong>",id:"integration-example",level:3},{value:"\ud83d\udcda Learn More",id:"-learn-more",level:2},{value:"\ud83c\udfaf Next Steps",id:"-next-steps",level:2}];function m(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components},{Details:r}=n;return r||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"\ufe0f-object-detection-pipeline",children:"\ud83d\udc41\ufe0f Object Detection Pipeline"})}),"\n",(0,i.jsxs)(n.p,{children:["Object detection enables your robot to ",(0,i.jsx)(n.strong,{children:"see and identify objects in the real world"}),", so it can pick, avoid, or interact with them.",(0,i.jsx)(n.br,{}),"\n","Your project uses a ",(0,i.jsx)(n.strong,{children:"YOLO-based detection pipeline"})," tightly integrated into ROS2 for real-time performance."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-object-detection-fundamentals",children:"\ud83e\udde0 Object Detection Fundamentals"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Object detection"})," combines ",(0,i.jsx)(n.strong,{children:"classification"})," and ",(0,i.jsx)(n.strong,{children:"localization"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Classification:"}),' "What is this object?" (bottle, cup, book, etc.)']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Localization:"}),' "Where is this object?" (bounding box, 3D coordinates)']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Confidence:"}),' "How sure are we?" (0.0 to 1.0 probability score)']}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"why-object-detection-matters",children:(0,i.jsx)(n.strong,{children:"Why Object Detection Matters"})}),"\n",(0,i.jsx)(n.mermaid,{value:"flowchart TD\r\n    Camera[Camera Feed] --\x3e Detection[Object Detection]\r\n    Detection --\x3e Class[Object Class<br/>bottle, cup, phone]\r\n    Detection --\x3e Location[2D Bounding Box<br/>x, y, width, height]\r\n    Detection --\x3e Depth[3D Position<br/>x, y, z in world]\r\n    \r\n    Class --\x3e Decision{Mission<br/>Decision}\r\n    Location --\x3e Decision\r\n    Depth --\x3e Decision\r\n    \r\n    Decision --\x3e|Pick Target| Manipulation[Arm Control]\r\n    Decision --\x3e|Obstacle| Navigation[Path Planning]\r\n    Decision --\x3e|Ignore| Continue[Continue Mission]"}),"\n",(0,i.jsx)(o.A,{type:"tip",title:"Real-Time Requirements",children:(0,i.jsxs)(n.p,{children:["Your pipeline must be ",(0,i.jsx)(n.strong,{children:"fast"})," (>10 FPS) and ",(0,i.jsx)(n.strong,{children:"reliable"}),", running directly on embedded hardware like Jetson Orin Nano."]})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-yolo-implementation",children:"\u26a1 YOLO Implementation"}),"\n",(0,i.jsxs)(n.p,{children:["Your project uses ",(0,i.jsx)(n.strong,{children:"YOLOv8"})," (You Only Look Once) for state-of-the-art real-time object detection."]}),"\n",(0,i.jsx)(n.h3,{id:"why-yolov8",children:(0,i.jsx)(n.strong,{children:"Why YOLOv8?"})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Feature"}),(0,i.jsx)(n.th,{children:"Benefit"}),(0,i.jsx)(n.th,{children:"Alternative"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Speed"})}),(0,i.jsx)(n.td,{children:"30+ FPS on edge devices"}),(0,i.jsx)(n.td,{children:"R-CNN (slower)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Accuracy"})}),(0,i.jsx)(n.td,{children:"mAP 50+ on COCO dataset"}),(0,i.jsx)(n.td,{children:"SSD (less accurate)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Efficiency"})}),(0,i.jsx)(n.td,{children:"Single-stage detection"}),(0,i.jsx)(n.td,{children:"Two-stage detectors"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"ROS2 Integration"})}),(0,i.jsx)(n.td,{children:"Easy Python binding"}),(0,i.jsx)(n.td,{children:"TensorFlow models"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"detection-pipeline-architecture",children:(0,i.jsx)(n.strong,{children:"Detection Pipeline Architecture"})}),"\n",(0,i.jsxs)(c.A,{children:[(0,i.jsx)(l.A,{value:"ros2",label:"ROS2 Integration",default:!0,children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# object_detection_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, PointCloud2\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom ultralytics import YOLO\r\nimport cv_bridge\r\n\r\nclass ObjectDetectionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('object_detection_node')\r\n        \r\n        # Load YOLOv8 model\r\n        self.model = YOLO('yolov8n.pt')  # or custom weights\r\n        self.bridge = cv_bridge.CvBridge()\r\n        \r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, '/camera/color/image_raw', \r\n            self.image_callback, 10)\r\n        self.depth_sub = self.create_subscription(\r\n            Image, '/camera/depth/image_rect_raw',\r\n            self.depth_callback, 10)\r\n            \r\n        # Publishers\r\n        self.detection_pub = self.create_publisher(\r\n            DetectionArray, '/detections', 10)\r\n        self.pose_pub = self.create_publisher(\r\n            PoseStamped, '/target_pose', 10)\r\n        self.viz_pub = self.create_publisher(\r\n            Image, '/detection_image', 10)\r\n    \r\n    def image_callback(self, msg):\r\n        # Convert ROS image to OpenCV\r\n        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\r\n        \r\n        # Run YOLO inference\r\n        results = self.model(cv_image)\r\n        \r\n        # Process detections\r\n        for r in results:\r\n            boxes = r.boxes\r\n            if boxes is not None:\r\n                self.process_detections(boxes, cv_image, msg.header)\r\n    \r\n    def process_detections(self, boxes, image, header):\r\n        detections = DetectionArray()\r\n        detections.header = header\r\n        \r\n        for box in boxes:\r\n            # Extract detection info\r\n            conf = float(box.conf[0])\r\n            cls = int(box.cls[0])\r\n            class_name = self.model.names[cls]\r\n            \r\n            # Filter by confidence\r\n            if conf > 0.5:  # Confidence threshold\r\n                detection = Detection()\r\n                detection.class_name = class_name\r\n                detection.confidence = conf\r\n                detection.bbox = self.extract_bbox(box)\r\n                \r\n                # Estimate 3D position if depth available\r\n                if self.latest_depth is not None:\r\n                    detection.pose = self.estimate_3d_pose(box, self.latest_depth)\r\n                \r\n                detections.detections.append(detection)\r\n        \r\n        # Publish results\r\n        self.detection_pub.publish(detections)\r\n        self.publish_visualization(image, detections)\n"})})}),(0,i.jsx)(l.A,{value:"pipeline",label:"Pipeline Flow",children:(0,i.jsx)(n.mermaid,{value:"graph TD\r\n    Camera[RGB/Depth Camera<br/>Orbbec DaBai] --\x3e Sync[Image Synchronization]\r\n    Sync --\x3e YOLO[YOLOv8 Inference<br/>GPU Accelerated]\r\n    \r\n    YOLO --\x3e Filter[Confidence Filter<br/>>0.5 threshold]\r\n    Filter --\x3e NMS[Non-Max Suppression<br/>Remove duplicates]\r\n    NMS --\x3e Depth[3D Pose Estimation<br/>Using depth data]\r\n    \r\n    Depth --\x3e Publish[Publish Detections]\r\n    Publish --\x3e Mission[Mission Manager]\r\n    Publish --\x3e Navigation[Navigation Stack]\r\n    Publish --\x3e Manipulation[Pick/Drop Nodes]\r\n    \r\n    YOLO -.->|Visualization| RViz[RViz Display]"})}),(0,i.jsx)(l.A,{value:"config",label:"Configuration",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# object_detection_config.yaml\r\nobject_detection:\r\n  model:\r\n    weights_path: "models/best.pt"\r\n    device: "cuda"  # or "cpu"\r\n    confidence_threshold: 0.5\r\n    nms_threshold: 0.4\r\n    \r\n  camera:\r\n    rgb_topic: "/camera/color/image_raw"\r\n    depth_topic: "/camera/depth/image_rect_raw"\r\n    camera_info_topic: "/camera/color/camera_info"\r\n    \r\n  publishing:\r\n    detection_topic: "/detections"\r\n    pose_topic: "/target_pose"\r\n    visualization_topic: "/detection_image"\r\n    publish_rate: 10.0  # Hz\r\n    \r\n  filtering:\r\n    target_classes: ["bottle", "cup", "book", "phone", "keys"]\r\n    min_area: 1000  # pixels\r\n    max_area: 50000  # pixels\n'})})})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-training-custom-models",children:"\ud83c\udfaf Training Custom Models"}),"\n",(0,i.jsx)(n.p,{children:"To detect new objects or improve accuracy for your specific environment:"}),"\n",(0,i.jsx)(n.h3,{id:"data-collection",children:(0,i.jsx)(n.strong,{children:"Data Collection"})}),"\n",(0,i.jsxs)(c.A,{children:[(0,i.jsxs)(l.A,{value:"collection",label:"Image Collection",default:!0,children:[(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Collect training images using your robot's camera\r\nros2 launch object_detection collect_training_data.launch.py\r\n\r\n# Or manually collect from saved bag files\r\nros2 bag play training_session.bag\r\nros2 run image_view image_saver image:=/camera/color/image_raw\n"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Best practices:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Collect ",(0,i.jsx)(n.strong,{children:"1000+ images"})," per object class"]}),"\n",(0,i.jsxs)(n.li,{children:["Include various ",(0,i.jsx)(n.strong,{children:"lighting conditions"})]}),"\n",(0,i.jsxs)(n.li,{children:["Show objects from ",(0,i.jsx)(n.strong,{children:"different angles"})]}),"\n",(0,i.jsxs)(n.li,{children:["Include ",(0,i.jsx)(n.strong,{children:"cluttered backgrounds"})]}),"\n",(0,i.jsxs)(n.li,{children:["Add ",(0,i.jsx)(n.strong,{children:"occlusion scenarios"})]}),"\n"]})]}),(0,i.jsxs)(l.A,{value:"annotation",label:"Annotation Process",children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Recommended tools:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Roboflow:"})," Web-based, excellent for YOLO formats"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CVAT:"})," Open-source, supports team collaboration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LabelImg:"})," Simple desktop tool for quick annotation"]}),"\n"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install LabelImg for local annotation\r\npip install labelimg\r\n\r\n# Launch annotation tool\r\nlabelimg /path/to/images /path/to/classes.txt\n"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Annotation format (YOLO):"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"# Each .txt file contains bounding boxes for one image\r\n# Format: class_id center_x center_y width height (normalized 0-1)\r\n0 0.5 0.3 0.2 0.4  # bottle at center-left\r\n1 0.8 0.7 0.15 0.25  # cup at bottom-right\n"})})]}),(0,i.jsxs)(l.A,{value:"training",label:"Model Training",children:[(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# train_custom_model.py\r\nfrom ultralytics import YOLO\r\n\r\n# Load pre-trained model\r\nmodel = YOLO('yolov8n.pt')\r\n\r\n# Train on custom dataset\r\nresults = model.train(\r\n    data='config/dataset.yaml',\r\n    epochs=100,\r\n    imgsz=640,\r\n    batch=16,\r\n    device='cuda',\r\n    save_period=10,\r\n    patience=20\r\n)\r\n\r\n# Export trained model\r\nmodel.export(format='onnx')  # For deployment optimization\n"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Dataset configuration:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# dataset.yaml\r\npath: /path/to/dataset\r\ntrain: images/train\r\nval: images/val\r\ntest: images/test\r\n\r\nnames:\r\n  0: bottle\r\n  1: cup\r\n  2: book\r\n  3: phone\r\n  4: keys\r\n  5: remote\n"})})]})]}),"\n",(0,i.jsx)(n.h3,{id:"model-deployment",children:(0,i.jsx)(n.strong,{children:"Model Deployment"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Copy trained weights to robot\r\nscp runs/detect/train/weights/best.pt robot@limo:/opt/ros2_ws/src/object_detection/models/\r\n\r\n# Update configuration\r\n# Edit object_detection_config.yaml to use new weights\r\nweights_path: "models/best.pt"\r\n\r\n# Restart detection node\r\nros2 launch object_detection detection.launch.py\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-detection-pipeline-integration",children:"\ud83d\udd17 Detection Pipeline Integration"}),"\n",(0,i.jsx)(n.h3,{id:"message-definitions",children:(0,i.jsx)(n.strong,{children:"Message Definitions"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Custom message: Detection.msg\r\nstring class_name\r\nfloat32 confidence\r\ngeometry_msgs/Point2D bbox_center\r\ngeometry_msgs/Point2D bbox_size\r\ngeometry_msgs/PoseStamped pose_3d\r\nbool has_3d_pose\r\n\r\n# Custom message: DetectionArray.msg\r\nstd_msgs/Header header\r\nDetection[] detections\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3d-pose-estimation",children:(0,i.jsx)(n.strong,{children:"3D Pose Estimation"})}),"\n",(0,i.jsxs)(c.A,{children:[(0,i.jsx)(l.A,{value:"depth",label:"Depth-based Estimation",default:!0,children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def estimate_3d_pose(self, bbox, depth_image, camera_info):\r\n    """Estimate 3D pose from 2D bounding box and depth data"""\r\n    \r\n    # Get bounding box center\r\n    center_x = int(bbox.center_x * depth_image.shape[1])\r\n    center_y = int(bbox.center_y * depth_image.shape[0])\r\n    \r\n    # Extract depth value (handle noise)\r\n    depth_region = depth_image[center_y-5:center_y+5, center_x-5:center_x+5]\r\n    depth_value = np.median(depth_region[depth_region > 0])\r\n    \r\n    if depth_value == 0:\r\n        return None  # Invalid depth\r\n    \r\n    # Convert to 3D coordinates using camera intrinsics\r\n    fx = camera_info.k[0]  # Focal length X\r\n    fy = camera_info.k[4]  # Focal length Y\r\n    cx = camera_info.k[2]  # Principal point X\r\n    cy = camera_info.k[5]  # Principal point Y\r\n    \r\n    # Calculate 3D position\r\n    x = (center_x - cx) * depth_value / fx\r\n    y = (center_y - cy) * depth_value / fy\r\n    z = depth_value\r\n    \r\n    # Create pose message\r\n    pose = PoseStamped()\r\n    pose.header.frame_id = "camera_depth_optical_frame"\r\n    pose.pose.position.x = x / 1000.0  # Convert mm to m\r\n    pose.pose.position.y = y / 1000.0\r\n    pose.pose.position.z = z / 1000.0\r\n    pose.pose.orientation.w = 1.0  # No rotation info from detection\r\n    \r\n    return pose\n'})})}),(0,i.jsx)(l.A,{value:"stereo",label:"Stereo Vision (Advanced)",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# For dual camera setup or stereo cameras\r\ndef stereo_triangulation(self, bbox_left, bbox_right, stereo_params):\r\n    """Calculate 3D position using stereo triangulation"""\r\n    \r\n    # Extract matching points from left and right images\r\n    x1, y1 = bbox_left.center_x, bbox_left.center_y\r\n    x2, y2 = bbox_right.center_x, bbox_right.center_y\r\n    \r\n    # Calculate disparity\r\n    disparity = x1 - x2\r\n    \r\n    if disparity <= 0:\r\n        return None  # Invalid disparity\r\n    \r\n    # Triangulate 3D point\r\n    baseline = stereo_params.baseline\r\n    focal_length = stereo_params.focal_length\r\n    \r\n    z = (baseline * focal_length) / disparity\r\n    x = (x1 - stereo_params.cx) * z / focal_length\r\n    y = (y1 - stereo_params.cy) * z / focal_length\r\n    \r\n    return [x, y, z]\n'})})})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-performance-optimization",children:"\ud83d\udcca Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"inference-speed",children:(0,i.jsx)(n.strong,{children:"Inference Speed"})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Optimization"}),(0,i.jsx)(n.th,{children:"Performance Gain"}),(0,i.jsx)(n.th,{children:"Trade-off"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"TensorRT"})}),(0,i.jsx)(n.td,{children:"2-3x faster"}),(0,i.jsx)(n.td,{children:"Setup complexity"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"ONNX Runtime"})}),(0,i.jsx)(n.td,{children:"1.5x faster"}),(0,i.jsx)(n.td,{children:"Moderate setup"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Model Pruning"})}),(0,i.jsx)(n.td,{children:"1.2x faster"}),(0,i.jsx)(n.td,{children:"Slight accuracy loss"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Lower Resolution"})}),(0,i.jsx)(n.td,{children:"2x faster"}),(0,i.jsx)(n.td,{children:"Reduced accuracy"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"memory-management",children:(0,i.jsx)(n.strong,{children:"Memory Management"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class OptimizedDetector:\r\n    def __init__(self):\r\n        # Pre-allocate memory for faster inference\r\n        self.input_tensor = torch.zeros((1, 3, 640, 640))\r\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n        \r\n        # Enable mixed precision for speed\r\n        self.scaler = torch.cuda.amp.GradScaler()\r\n        \r\n    def detect_optimized(self, image):\r\n        with torch.cuda.amp.autocast():\r\n            # Preprocess image in-place\r\n            preprocessed = self.preprocess_inplace(image)\r\n            \r\n            # Run inference\r\n            with torch.no_grad():\r\n                results = self.model(preprocessed)\r\n                \r\n            return self.postprocess(results)\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"\ufe0f-troubleshooting--best-practices",children:"\ud83d\udee0\ufe0f Troubleshooting & Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues",children:(0,i.jsx)(n.strong,{children:"Common Issues"})}),"\n",(0,i.jsxs)(r,{children:[(0,i.jsxs)("summary",{children:["\ud83c\udfaf ",(0,i.jsx)(n.strong,{children:"Low Detection Accuracy"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Possible causes:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Poor lighting conditions"}),"\n",(0,i.jsx)(n.li,{children:"Object partially occluded"}),"\n",(0,i.jsx)(n.li,{children:"Model not trained on similar objects"}),"\n",(0,i.jsx)(n.li,{children:"Camera focus/calibration issues"}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Solutions:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Improve preprocessing\r\ndef enhance_image(self, image):\r\n    # Histogram equalization for lighting\r\n    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\r\n    lab[:,:,0] = cv2.createCLAHE(clipLimit=2.0).apply(lab[:,:,0])\r\n    enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\r\n    \r\n    # Gaussian blur to reduce noise\r\n    enhanced = cv2.GaussianBlur(enhanced, (3, 3), 0)\r\n    \r\n    return enhanced\r\n\r\n# Multi-scale detection\r\ndef multi_scale_detect(self, image):\r\n    results = []\r\n    scales = [0.8, 1.0, 1.2]\r\n    \r\n    for scale in scales:\r\n        resized = cv2.resize(image, None, fx=scale, fy=scale)\r\n        result = self.model(resized)\r\n        results.extend(result)\r\n    \r\n    return self.merge_results(results)\n"})})]}),"\n",(0,i.jsxs)(r,{children:[(0,i.jsxs)("summary",{children:["\u26a1 ",(0,i.jsx)(n.strong,{children:"Performance Issues"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Frame rate too low:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Optimize detection frequency\r\nclass AdaptiveDetector:\r\n    def __init__(self):\r\n        self.detection_interval = 3  # Process every 3rd frame\r\n        self.frame_count = 0\r\n        self.last_detections = []\r\n        \r\n    def process_frame(self, image):\r\n        self.frame_count += 1\r\n        \r\n        if self.frame_count % self.detection_interval == 0:\r\n            # Run detection\r\n            self.last_detections = self.detect(image)\r\n        \r\n        # Always return last known detections\r\n        return self.last_detections\n"})})]}),"\n",(0,i.jsxs)(r,{children:[(0,i.jsxs)("summary",{children:["\ud83d\udd27 ",(0,i.jsx)(n.strong,{children:"Integration Issues"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Detection not triggering actions:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Debug ROS2 communication\r\nros2 topic list | grep detection\r\nros2 topic echo /detections --once\r\nros2 node info /object_detection_node\r\n\r\n# Check message flow\r\nros2 run rqt_graph rqt_graph\r\n\r\n# Monitor detection frequency\r\nros2 topic hz /detections\n"})})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-advanced-features",children:"\ud83d\udcc8 Advanced Features"}),"\n",(0,i.jsx)(n.h3,{id:"object-tracking",children:(0,i.jsx)(n.strong,{children:"Object Tracking"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class ObjectTracker:\r\n    def __init__(self):\r\n        self.tracker = cv2.TrackerCSRT_create()\r\n        self.tracking_objects = {}\r\n        \r\n    def update_tracking(self, image, detections):\r\n        # Initialize trackers for new objects\r\n        for detection in detections:\r\n            if detection.confidence > 0.8:  # High confidence objects\r\n                tracker_id = self.start_tracking(image, detection.bbox)\r\n                self.tracking_objects[tracker_id] = detection\r\n        \r\n        # Update existing trackers\r\n        for tracker_id, tracker in self.tracking_objects.items():\r\n            success, bbox = tracker.update(image)\r\n            if success:\r\n                self.update_object_pose(tracker_id, bbox)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"multi-class-strategy",children:(0,i.jsx)(n.strong,{children:"Multi-Class Strategy"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Priority-based detection for mission planning\r\nOBJECT_PRIORITIES = {\r\n    'target_item': 1,      # Highest priority\r\n    'obstacle': 2,         # Avoid these\r\n    'reference': 3,        # For localization\r\n    'background': 4        # Ignore these\r\n}\r\n\r\ndef prioritize_detections(self, detections):\r\n    # Sort by priority, then confidence\r\n    return sorted(detections, \r\n                 key=lambda d: (OBJECT_PRIORITIES.get(d.class_name, 5), \r\n                               -d.confidence))\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-output-topics--integration",children:"\ud83d\udcca Output Topics & Integration"}),"\n",(0,i.jsx)(n.h3,{id:"published-topics",children:(0,i.jsx)(n.strong,{children:"Published Topics"})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Topic"}),(0,i.jsx)(n.th,{children:"Message Type"}),(0,i.jsx)(n.th,{children:"Frequency"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/detections"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"DetectionArray"})}),(0,i.jsx)(n.td,{children:"10 Hz"}),(0,i.jsx)(n.td,{children:"All detected objects"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/target_pose"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"PoseStamped"})}),(0,i.jsx)(n.td,{children:"On detection"}),(0,i.jsx)(n.td,{children:"3D pose of target object"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/detection_image"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"Image"})}),(0,i.jsx)(n.td,{children:"10 Hz"}),(0,i.jsx)(n.td,{children:"Visualization with bounding boxes"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/detection_status"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"std_msgs/String"})}),(0,i.jsx)(n.td,{children:"1 Hz"}),(0,i.jsx)(n.td,{children:"System status and errors"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"integration-example",children:(0,i.jsx)(n.strong,{children:"Integration Example"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# mission_manager.py - Using detections\r\nclass MissionManager(Node):\r\n    def __init__(self):\r\n        self.detection_sub = self.create_subscription(\r\n            DetectionArray, '/detections', \r\n            self.detection_callback, 10)\r\n            \r\n    def detection_callback(self, msg):\r\n        for detection in msg.detections:\r\n            if detection.class_name == \"target_bottle\":\r\n                if detection.confidence > 0.8:\r\n                    # Trigger pick sequence\r\n                    self.request_pickup(detection.pose_3d)\r\n                    break\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-learn-more",children:"\ud83d\udcda Learn More"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.ultralytics.com/",children:"YOLOv8 Official Documentation"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Tutorials/Advanced/Computer-Vision.html",children:"ROS2 Computer Vision Tutorials"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html",children:"OpenCV Python Documentation"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://roboflow.com/",children:"Roboflow Training Platform"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173",children:"Object Detection Metrics Guide"})}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-next-steps",children:"\ud83c\udfaf Next Steps"}),"\n",(0,i.jsxs)(o.A,{type:"note",title:"Ready for Integration?",children:[(0,i.jsx)(n.p,{children:"Now that you understand object detection:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integration:"})," ",(0,i.jsx)(n.a,{href:"/limo_cobot_documentation/docs/core-concepts/system-integration",children:"System Integration"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implementation:"})," ",(0,i.jsx)(n.a,{href:"/limo_cobot_documentation/docs/system-components/object-detection-node",children:"Object Detection Node"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Advanced:"})," ",(0,i.jsx)(n.a,{href:"/limo_cobot_documentation/docs/advanced-usage/custom-objects",children:"Custom Object Training"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Troubleshooting:"})," ",(0,i.jsx)(n.a,{href:"/limo_cobot_documentation/docs/troubleshooting/debugging-guide",children:"Detection Issues"})]}),"\n"]})]})]})}function x(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}}}]);